{"cells":[{"cell_type":"markdown","metadata":{"id":"jbPCAJ7abfJW"},"source":["\n","# Week 11 Attention and Transformers\n","\n","Proprietary material - Under Creative Commons 4.0 licence CC-BY-NC https://creativecommons.org/licenses/by-nc/4.0/legalcode"]},{"cell_type":"markdown","metadata":{},"source":["# Introduction\n","\n","In the last decade or so, there has been another change in paradigm for NLP with the introduction of transformers and attention models. Now, attention is a complex operation that might be a bit hard to understand at the beginning. But let's try to explain it step by step to make it as intuitively as possible.\n","\n","# Encoder/Decoder\n","\n","To understand attention, we need to explain a bit of the context where it was conceptualized.\n","\n","Let's remember the previous section where we talked about sequential models. In particular about the sequence to sequence models like the following illustration:\n","\n","<img src=\"RNNs-Page-5.drawio.png\">\n","\n","A generalization of this structure is called an encoder-decoder model. There are several types of encoder-decoders with diferent applications, but their general forms follows something like this:\n","\n","<img src=\"RNNs-Page-12.png\">\n","\n","The job of the encoder is to create a vector representation of the input. This vector is often called the context vector (C) that contains the relevant information from the input. Then, the decoder interprets this context vector to produce the output of the model. \n","\n","### Autoencoders\n","\n","It's a bit of a deviation, but let's quickly mention autoencoders. Structurally, they are the same as the encoder-decoders, but instead of decoding to a different space they reconstruct the original input. This creates a projection in a latent space in the context vector that represents the input in a reduced dimensionality. \n","\n","A common example of this is the projection of the mnist digits on a 2d plane:\n","\n","<img src=\"autoencoder_schema.jpg\">\n","\n","<img src=\"latent_space.png\">\n","\n","This isn't pertinent to attention, but couldn't find somewhere else to explain it.\n","\n","## Machine Translation\n","\n","Let's get back in track by seeing an example of machine translation with encoder-decoders.\n","\n","<img src=\"RNNs-Page-13.png\">\n","\n","Now, we see that the only communication from the coder and the decoder is the context vector. There for is the biggest bottleneck of information in the model. This is a glaring issue, as the quality of this context vector will directly impact the quality of the translation.\n","\n","To try to address this issue is how the original attention operator was designed.\n","\n","# Taking Attention\n","\n","As we said in the previous week material, RNNs in general have a tendency of forgetting previous observations. This is particularly the case in machine translation, where the model would forget the beginning of the sentence by the end. LSTM and GRU models where introduced to fix this issue, but they still struggled with long sentences. \n","\n","To fix this, Attention was proposed to allow the model to focus on the relevant part of the whole sentence. \n","\n","To start, we need to make some changes to our encoder. Instead of ditching all the hidden states and keeping the last one, we can combine them on to a context matrix like so\n","\n","<img src=\"RNNs-Page-14.png\">\n","\n","Now we can feed this matrix to the decoder. But with attention, the decoder has to calculate the attention of the inputs for each cell. This is done in the following steps\n","\n","1. Grab the input matrix associating each vector column to a hidden state from the encoder and a hidden state from the decoder.  \n","2. Calculate the attention score for each vector in reference to the hidden state. In theory, this can be any function that grabs two vectors and returns a value. In practice, this is done by a fc layer but there are several proposals on how to apply attention\n","3. Get the softmax value for the attention scores and multiply them to their corresponding hidden states. This allows the model to dynamically give focus to different hidden states and ignore less important ones.\n","4. Finally, we add all the weighted states together and end up with a context vector for this time step\n","\n","<img src=\"RNNs-Page-15.png\">\n","\n","Now we can operate the attention block inside the decoder by following the next steps:\n","\n","1. Take the previous output $y_{i-1}$ and hidden state $h_{i-1}$ and pass them trough the RNN cell to create a new hidden state $h_i$ for this time step. In case it's the first cell, use the end token as an input and a default hidden state $h_0$\n","2. Take attention to the context matrix using the new hidden state and generate a context vector $C_i$\n","3. Concatenate both the context vector $C_i$ and the hidden vector $h_i$ together \n","4. Pass the resulting vector trough a small fully connected network  (two layers is usually enough) to get the output for this time step\n","5. Repeat for the next time steps until the end token is received \n","\n","<img src=\"RNNs-Page-16.png\">\n","\n","With that, we have completed our architecture with attention! \n","\n","We can see an example of machine translation with attention in the next figure:\n","\n","<img src=\"attention_sentence.png\">\n","\n","As you can see, attention even allows the model to understand when words are flipped in a sentence while translating. This power to understand complex sequences is what separated attention models from the rest. \n","\n","But even attention has some problems. First, like any sequential model it's slow to evaluate and train. This is because you still need the next hidden state to calculate the attention scores. This also means that the score and therefore the attention calculation depends heavily on the quality of the hidden states. \n","\n","# Self-Attention\n","\n","What if instead of relying on the hidden state of a RNN, we could ask the data itself where the relevant information and correlations are? \n","\n","This is where the concept of self-attention comes into place. Not only improved on the performance of the original attention models, but it allowed models to operate sequences in parallel. \n","\n","To apply self-attention we can't just feed two times the attention score calculation. Instead we need to do the following steps. I'll start the explanation with a single input vector and show how it looks with a whole matrix at the end. Just consider that for the next couple of steps.\n","\n","\n","1. We need to start by calculating the Queries, Keys and Values for each of the sequence inputs. This is done by multiplying the input with $W^Q$, $W^K$ and $W^V$ to to end up with the vectors $q$, $k$ and $v$ respectively. This vectors are abstractions of our input that allow us to calculate attention\n","\n","\n","<img src=\"RNNs-Page-17.png\">\n","\n","2. Next, we need to calculate the attention score for each observation to the rest of the sequence. This is done by taking the dot product between the query vector of the observation $x_i$ that we want and the key vectors of the whole sequence\n","\n","$$\n","q_i \\cdot k_j^T\n","$$\n","\n","3. Once we have the score we have to normalize it by the square of the length of the key vector. This is done to stabilize the gradients and ease the learning process. For our example this would be the square root of 3, but in the original paper the length of k is 64, so they divide by 8\n","\n","$$\n","\\frac{q_i \\cdot k_j^T}{\\sqrt{d_k}}\n","$$\n","\n","4. Then, we the softmax value of the scores for the whole input to normalize the sum of all values to 1\n","\n","$$\n","s_j = Softmax(\\frac{q_i \\cdot k_j^T}{\\sqrt{d_k}}) \n","$$\n","\n","5. Then, we multiply each value vector by their corresponding attention scores\n","\n","$$\n","V_j*s_j\n","$$\n","\n","6. Finally, we add all the weighted value vectors to end up with the self-attention vector for our corresponding observation $x_i$\n","\n","$$\n","z_i = \\sum(V_j s_j)\n","$$\n","\n","Then,  we could repeat the process for each of the input sequences to get the whole self-attention matrix, but we can use matrix multiplications to do this calculations very efficiently in parallel. Now we define the Query, Keys and Values as so:\n","\n","<img src=\"RNNs-Page-18.png\">\n","\n","Which leave the final attention calculation like\n","\n","<img src=\"RNNs-Page-19.png\">\n","\n","In equation form\n","\n","$$\n","Q = X W^Q\n","$$\n","\n","$$\n","K = X W^K\n","$$\n","\n","$$\n","V = X W^V\n","$$\n","\n","$$\n","Z = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V\n","$$\n","\n","\n","Let's see how self-attention takes focus with an example.\n","\n","\n","\"The animal didn't cross the street because it was too tired\"\n","\n","What does \"it\" refer in this sentence? \n","\n","For humans this clearly refers to the animal, but previous models found difficult to get this correlation. let's see how self-attention sees it:\n","\n","<img src=\"selfattention.png\">\n","\n","Here we can see that the model is capable of making the connection to know that \"it\" refers to the animal.\n","\n","\n","# Multi Headed Self-Attention\n","\n","Now that we have the attention operator defined are we done with our model? \n","\n","No, we are not limited to a single operator of self-attention. We refer to each of this operators as a \"head\", so using multiple operators is referred as multi-headed self-attention.\n","\n","This has the advantage of allowing each head to focus in specific positions or different contexts. It also gives the multi-headed layer the ability to generate different representation subspaces that can project the embeddings to a different space for deeper layers.\n","\n","An example of the same word on the same sentence but with multi-headed self attention can be seen here:\n","\n","\n","<img src=\"mhsa.png\">\n","\n","If you want to explore in more detail how attention is taken with interactive examples I would recommend this two references\n","\n","https://github.com/jessevig/bertviz\n","\n","https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb#scrollTo=OJKU36QAfqOC\n","\n","\n","\n","# Transformer\n","\n","As a last point for this session, let's talk about transformers!\n","\n","\n","Transformers are an architecture that was proposed next to self-attention in the paper \"Attention is all you need\". A very influential paper that I would highly recommend checking out!\n","\n","The general architecture of the transformer can be seen in the following scheme:\n","\n","<img src=\"transformer_small.png\">\n","\n","And with that we are finished!\n","\n","There are some details about the implementation that we haven't covered, but this should cover the fundamental idea behind attention. If you want to deepen your knowledge about transformers I would higly recommend implementing them from scratch wit Pytorch. \n","\n"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Week 11 CNNs.ipynb","provenance":[{"file_id":"1QwT_X2A4mgg5E0jbBl7mDnFdiFTQOJlM","timestamp":1611160170561}],"toc_visible":true},"kernelspec":{"display_name":"Python 3.6.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9 (default, Jun 29 2022, 11:45:57) \n[GCC 8.4.0]"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
