{"cells":[{"cell_type":"markdown","metadata":{"id":"jbPCAJ7abfJW"},"source":["\n","# Week 10 Recurrent Nets\n","\n","Proprietary material - Under Creative Commons 4.0 licence CC-BY-NC https://creativecommons.org/licenses/by-nc/4.0/legalcode"]},{"cell_type":"markdown","metadata":{"id":"bEVtC55FikuK"},"source":["# Sequential Data\n","\n","Before jumping on to recurrent neural nets let's make a quick definition of sequential data.\n","\n","Sequential data is any data structure that is structured sequentially. This means that are several data samples or observations ordered in sequence. The order of the data on itself provides a great deal of the information. \n","\n","Some common examples of sequential data is text data, genetic sequencing, time series (weather, stock prices, etc), video data and audio data. \n","\n","This type of data has some peculiarities that makes it hard to handle using traditional machine learning methods or deep learning. First, sequential data can have variable lengths. If we compare it to image data, the size of the images is always the same. But for sequential data we can't make tha assumption tha easily. \n","\n","For example with text:\n","\n","> \"My dog loves the park\" -> [\"My\", \"dog\", \"loves\", \"the\", \"park\", \"\\end\"] -> length: 6\n","\n","> \"The quick brown fox jumps over the lazy dog\" -> [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \"\\end\"] -> length: 10\n","\n","If we wanted to feed this sequences to a traditional neural net, we would soon find ourselves with a size mismatch. A first approach could be to pad the shorter sequences to match the longest sequence in our data. Going back to our text example, it would look like this:\n","\n","> \"My dog loves the park \\null \\null \\null \\null\" -> [\"My\", \"dog\", \"loves\", \"the\", \"park\", \"\\null\", \"\\null\", \"\\null\", \"\\null\", \"\\end\"] -> length: 10\n","\n","This approach has some problems. First, it's not easy to determine the max length to pad. Making an assertion of the max length in unseen data it's a strong assertion to make and does not always work. Second, adding too much padding to the sentences adds a very strong bias to the model to predict null values. In practice this approach also doesn't work very well.\n","\n","A second approach could be to just iterate the model over each of the sequence samples. For example, let's assume we have a model that receives a spanish word and translates it to english. if we translated some text like this:\n","\n","    \"Yo no fui quien saco al perro chico\"\n","-> model ->\n","\n","    \"I no was who outed the dog small\" \n","\n","Clearly this approach doesn't work very well. This is because we are ignoring the surrounding context of the samples in the sequence. Trying to fix both of this problems is how the recurrent models where conceptualized. \n","  \n"]},{"cell_type":"markdown","metadata":{},"source":["# Recurrent Models\n","\n","The basic definition of recurrent models is that they are models that use it's previous outputs to generate it's next one. When working with neural nets we refer to them as Recurrent Neural Networks (RNN). \n","\n","\n","There are 5 general RNNs types: \n","\n","- One-to-one:\n","\n","$L_x = L_y = 1$\n","\n","<img src=\"RNNs-Page-1.drawio.png\">\n","\n","This would be analogous to a normal neural network.\n","\n","- One-to-many:\n","\n","$L_x = 1, L_y > 1$\n","\n","<img src=\"RNNs-Page-2.drawio.png\">\n","\n","This type can be used to generate text or music.\n","\n","- Many-to-one:\n","\n","$L_x > 1, L_y = 1$\n","\n","<img src=\"RNNs-Page-3.drawio.png\">\n","\n","An example of this type is spam or sentiment classification in text or music genre for audio. \n","\n","- Many-to-many:\n","\n","$L_x = L_y > 1$\n","\n","<img src=\"RNNs-Page-4.drawio.png\">\n","\n","Can be used for weather prediction, time series classification or feature extraction.\n","\n","- Many-to-many:\n","\n","$L_x != L_y > 1$\n","\n","<img src=\"RNNs-Page-5.drawio.png\">\n","\n","This is usually used for machine translation.\n","\n","We refer to $a_n$ as the hidden state of the model. Notice that at the start we always give the model a default hidden state called $a_0$. This is to indicate to the model that it's at the start of the sequence. \n","\n","\n","> Note: VERY IMPORTANT\n","\n","Most confusion when starting to study RNNs is that there are multiple RNN blocks that are feed in order. In reality, its the same RNN block being reused constantly. There are some exemptions, like in coder-decoder architectures, where there are two. But it's the same weights are reused across the inputs and outputs. Another way to see them is by the following diagram:\n","\n","<img src=\"RNNs-Page-6.drawio.png\">\n"]},{"cell_type":"markdown","metadata":{},"source":["# RNN Architecture\n","\n","The inside of a basic RNN block is composed like so:\n","\n","<img src=\"RNNs-Page-7.drawio.png\">\n","\n","Here, the outputs are expressed by the following equations:\n","\n","$$\n","a_{n} = g_1(W_{aa}a_{n-1} + W_{ax}x_{n} + b_a)\n","$$\n","\n","$$\n","y_n = g_2(W_{ya}a_{n} + b_y)\n","$$\n","\n","Where $W_{aa}, W_{ax}, W_{ya}, b_a, b_y$ are the weights and bias of fully connected layers and $g_1, g_2$ are activation functions. The most common activation functions for RNNs are sigmoid, tanh and ReLu. \n","\n","Now, this type of architecture has some advantages and disadvantages: \n","\n","#### Advantages:\n","- Can process any input length\n","- Model size does not depend on input length \n","- Predictions take in account previous information \n","\n","#### Disadvantages:\n","- Sequential operation makes it slow to evaluate and train\n","- Tendency to quickly forget far away samples\n","- Can only consider previous samples for it's output \n","\n","## RNN Architectures\n","\n","Now, there are some basic architectures made with RNNs:\n","\n","### Bidirectional RNN (BRNN)\n","\n","BRNNs try to answer the problem of only considering previous samples by operating the sequence front to back and back to front like so:\n","\n","<img src=\"RNNs-Page-8.drawio.png\">\n","\n","### Deep RNN (DRNN)\n","\n","DRNN are a stack of RNN blocks that allow it to extract features for deeper RNN blocks in the architecture similarly to a regular deep learning model.\n","\n","<img src=\"RNNs-Page-9.drawio.png\">"]},{"cell_type":"markdown","metadata":{},"source":["# Long Short-Term Memory (LSTM)\n","\n","The LSTM model is a general improvement of the classic RNN models. Instead of just relying on the hidden state, LSTM introduces a short memory storage in the form of weights. For this, each LSTM cell has an internal state and three gates that determine how can be affected:\n","\n","1. Input gate: Whether the input should affect the internal state\n","2. Forget gate: If the internal state should be forgotten \n","3. Output gate: If the internal state should affect the output\n","\n","<img src=\"RNNs-Page-10.drawio.png\">\n","\n","Where $I$ is the input gate, $F$ is the forget gate, $O$ is the output gate and $\\hat{C}$ is the input node.\n","\n","\n","# Gated Recurrent Units (GRU)\n","\n","GRUs are similar to LSTMs but instead of having three gates they have the following two:\n","\n","1. Reset gate: How much of the previous hidden state is remembered\n","2. Update gate: How much the old hidden state overwrites the new one\n","\n","<img src=\"RNNs-Page-11.drawio.png\">\n","\n","Where $R$ is the reset gate, $Z$ is the update gate and $H$ is the candidate hidden state.\n","\n","\n","We can also create architectures with LSTMs and GRUs just like with RNNs. This results on BLSTM/BGRU and DLSTM/DGRU."]},{"cell_type":"markdown","metadata":{},"source":["# NLP\n","\n","Now, what we have talked up to now can be applied to any type of sequential data. But working with natural language processing (NLP) models is among the most popular and influential disciplines in the machine learning community. We don't have the time to give it a proper introduction, so i'll just cover some basics on how to use NLP for sequential models. \n","\n","## Tokenization\n","\n","We saw a bit of tokenization when we made a spam classifier but didn't introduce it properly. Basically, it's the process by which we separate a string of text on to smaller \"tokens\". Some common ways of tokenization are: \n","\n","- Word tokenization: Each token is a word from the text separated by natural breaks like spaces, breaks or punctuation.\n","\n","> \"going to the nearest park\" -> tokenizer -> \"going\", \"to\", \"the\", \"nearest\", \"park\"\n","\n","- Character tokenization: Each word of the phrase is tokenized. \n","\n","> \"going to the nearest park\" -> tokenizer -> \"g\", \"o\", \"i\", \"n\", \"g\", \" \", \"t\", \"o\", \" \", \"t\", \"h\", \"e\", \" \", \"n\", \"e\", \"a\", \"r\", \"e\", \"s\", \"t\", \" \", \"p\", \"a\", \"r\", \"k\"\n","\n","- Subword tokenization: Each token is the word separated from its prefixes and suffixes.\n","\n","> \"going to the nearest park\" -> tokenizer -> \"go\", \"ing\", \"to\", \"the\", \"near\", \"est\", \"park\"\n","\n","## Word Representation\n","\n","Now that we have our tokens we need to have a way to transform them to a numerical representation that our model can understand.\n","\n","### Vocabulary\n","\n","The most basic representation, it just replaces a token with an associated number from a dictionary.\n","\n","> [\"one\", \"for\", \"all\", \"all\", \"for\", \"one\"] -> vocab -> [0, 1, 2, 2, 1, 0]\n","\n","With vocab being the dictionary: {\"one\" : 0, \"for\" : 1, \"all\" : 2}\n","\n","Now, this has the problem that the words have different distances for it's representations. This means that the model sees the distance between words and associates that nearby words have more in common than far away words. \n","\n","### One-hot\n","\n","Instead of representing by a single number, we can represent the words by a one-hot encoding. \n","\n","> [\"one\", \"for\", \"all\", \"all\", \"for\", \"one\"] -> vocab -> [0, 1, 2, 2, 1, 0] -> one-hot -> [[1,0,0], [0,1,0], [0,0,1], [0,0,1], [0,1,0], [1,0,0]]\n","\n","This has the advantage that all the words are at the same hamilton distance from each other, so we avoid adding a wrong bias to the model. Now, we might reason that some words are similar and they should be closer to each other. \n","\n","### Word Embedding\n","\n","The word embeddings are a vector representation of the words similarly to one-hot. But word embeddings have the advantage that it takes in account the similarity between words. Some common models for word embedding are word2vec, skip-gram, negative sampling and GloVe. \n","\n","<img src=\"onehot_embedding.png\">\n"]},{"cell_type":"markdown","metadata":{},"source":["# Coder-Decoder"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Week 11 CNNs.ipynb","provenance":[{"file_id":"1QwT_X2A4mgg5E0jbBl7mDnFdiFTQOJlM","timestamp":1611160170561}],"toc_visible":true},"kernelspec":{"display_name":"Python 3.7.13 ('yolor37')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"vscode":{"interpreter":{"hash":"7c52148af6cc9cef38b35fc55a92f2a069d363a9a0497e2c12237ff32726f011"}}},"nbformat":4,"nbformat_minor":0}
