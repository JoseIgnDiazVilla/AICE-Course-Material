{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Models\n",
    "\n",
    "Proprietary material - Under Creative Commons 4.0 licence CC-BY-NC-ND https://creativecommons.org/licenses/by-nc-nd/4.0/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a heads up, in this material we will give a very quick brush over of Bayesian models. Just giving a reasonable introduction to the subject could take a whole semester with a proper understanding of probability and statistics and truly grasping bayesian thinking can take even longer. \n",
    "\n",
    "Still, I think it can be valuable to at least go over some basic concepts to explain the strengths and values of bayesian models. At the end, I will leave several references that I highly recommend if you are interested in the subject. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Thinking\n",
    "\n",
    "Statistics is the discipline that concerns the organization, analysis, interpretation and presentation of data. Statistical thinking is the systematic way of thinking how we describe the world using data to make predictions and decisions.\n",
    "\n",
    "\n",
    "Just like statistical thinking, our human intuition often tries to answer questions of the reality around us. The main difference is that our intuition often provides the wrong answers.\n",
    "\n",
    "For example, in the United States there has been reported that more more people think that violent crime has been on the rise in recent years. However, a statistical analysis of the data indicates that violent crime has been decreasing since the 1990's. \n",
    "\n",
    "Our intuition is often wrong because we rely upon guesses shaped by our biases and experiences that doesn't apply in general.\n",
    "\n",
    "\n",
    "There are three major thinks that we can do with statistics:\n",
    "\n",
    "1. **Describe**: The world is complex and we need a simplification to understand it.\n",
    "2. **Decide**: We often need to make decision based on data, even if we have uncertainty.\n",
    "3. **Predict**: We often need to make predictions about new situations based on previous data.\n",
    "\n",
    "\n",
    "\n",
    "> Ref [Poldrack, 2019]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability and Statistics\n",
    "\n",
    "Probability is the language of uncertainty that is also the basis for statistical inference. It forms an important part of the foundation for statistics, because it provides us with the mathematical tools to describe uncertain events.\n",
    "\n",
    "\n",
    "The problem studied in probability is: \n",
    "\n",
    "Given a data generating process, which are the properties of the outputs?\n",
    "\n",
    "We can compare this to the problem that we study in statistical inference, data mining and machine learning:\n",
    "\n",
    "Given the outputs, what can we say about the process that generates the observed data?\n",
    "\n",
    "\n",
    "We think about probability as a number (from 0 to 1) that describes the likelihood of an event occurring. This indicates how likely that particular event occurrence is, ranging from impossible to guaranteed.\n",
    "\n",
    "### Some Important Concepts\n",
    "\n",
    "- Random Experiment: \n",
    "The act of measuring a process whose output is uncertain. Ej: flipping a coin, predicting travel arrival time, etc.\n",
    "\n",
    "- Sample Space $\\Omega$:\n",
    "The set with all posible outputs of a random experiment. It can either be discrete or continuous. \n",
    "\n",
    "Ej: For a six sided dice, the sample space is: \n",
    "\n",
    "$\\Omega = \\{1, 2, 3, 4, 5, 6\\}$\n",
    "\n",
    "- Event $E$:\n",
    "It's a subset of the sample space.\n",
    "\n",
    "Ej: Going back to the dice, an event would be getting an even number like so:\n",
    "\n",
    "$E = \\{2, 4, 6\\}$\n",
    "\n",
    "## Probability\n",
    "\n",
    "A formal definition of probability $P$ is a real-valued function defined over $\\Omega$ that satisfies the following properties:\n",
    "\n",
    "1. For any event $E \\subseteq \\Omega, 0 \\leq P(E) \\leq 1$\n",
    "2. The probability of tha sample space is 1. $P(\\Omega)$\n",
    "3. Let $E_1, E_2, ..., E_k \\in \\Omega$ be disjoint sets\n",
    "\n",
    "$$\n",
    "P(\\bigcup^{k}_{i=1} E_i) = \\sum^k_i P(E_i)\n",
    "$$\n",
    "\n",
    "Now we have a definition, but how do we interpret it?\n",
    "\n",
    "There are two main ways to interpret probabilities: frequentist and bayesian.\n",
    "\n",
    "### Frequentist probability\n",
    "\n",
    "For the frequancy interpretation, $P(E)$ is the long run proportion of times that $E$ is true.\n",
    "\n",
    "For example, let's say that the probability of throwing a coin and getting heads is 1/2, we mean that if we flip the coin many times then the proportion of cases that results in heads tends to 1/2 as the number of coin tosses increases. \n",
    "\n",
    "When the sample space $\\Omega$ is finite, we can say that \n",
    "\n",
    "$$\n",
    "P(E) = \\frac{Favorable Cases}{Total Cases} = \\frac{|E|}{|\\Omega|}\n",
    "$$\n",
    "\n",
    "### Bayesian probability\n",
    "\n",
    "The bayesian approach (or degree of belief interpretation) is that $P(E)$ measures an observer's strength of belief that $E$ is true. If I ask \"How likely is that a political candidate will get elected\", you can provide an answer based on your knowledge and beliefs even if there are no relevant frequencies to compute a frequentist probability.\n",
    "\n",
    "\n",
    "The difference in interpretation doesn't usually matter much until we deal with inference. Then, this difference in approach leads to the two schools of inference: the frequentist and Bayesian schools. \n",
    "\n",
    "As I said before, there are too many things to go over to explain bayesian models properly, still I will quickly explain some of the most relevant for now.\n",
    "\n",
    "### Conditional Probabilities\n",
    "\n",
    "The conditional probability of $Y$ given $X$ is defined as:\n",
    "\n",
    "$$\n",
    "P(Y|X) = \\frac{P(X,Y)}{P(X)}\n",
    "$$\n",
    "\n",
    "$P(Y|X)$ can be interpreted as the fraction of times $Y$ occurs when $X$ is known to occur.\n",
    "\n",
    "If $X$ and $Y$ are independent then $P(Y|X) = P(Y)$\n",
    "\n",
    "If this isn't clear, please let me know and we can see an example on the whiteboard. \n",
    "\n",
    "### Bayes Theorem\n",
    "\n",
    "The conditional probability $P(Y|X)$ and $P(X|Y)$ can be expressed as a function of each other using bayes theorem:\n",
    "\n",
    "$$\n",
    "P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Distributions\n",
    "\n",
    "A probability distribution is a function that indicates the likelihood of occurrence that a random variable will take certain values over the sample space. The distributions can be either continuous or discrete, but the integral over the sample space always has to be 1. \n",
    "\n",
    "Some of the more important distribution functions are: \n",
    "\n",
    "<img src=\"pdfs.png\">\n",
    "\n",
    "Mainly, the binomial distribution is a discrete function that indicates how likely is to get a number of successes over a number of trials.\n",
    "\n",
    "The Normal or Gaussian distribution it's very important in statistics because it shows up all the time in nature. \n",
    "\n",
    "<img src=\"normalpdf.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Inference \n",
    "\n",
    "The main goal of statistical inference is to investigate some properties about a target population.\n",
    "\n",
    "We define a population as the entire group of individuals that we are interested in studying. This can be anything from all humans to a type of cell or some particles. \n",
    "\n",
    "Each individual in a population is often called an unit.\n",
    "\n",
    "Often, to draw conclusions about a population, it's not feasible to gather the data from all the population. But when we can this is called a census. If we extract a subgroup of the population this is called a sample.\n",
    "\n",
    "In statistical inference we try to make reasonable conclusions about a population based on the evidence provided by a sample data.\n",
    "From a more general point of view, the goal of the inference is to infer the distribution that generated the observed data. For example, if we have a sample $X_1, ..., X_n \\sim F$, how do we infer F?\n",
    "\n",
    "But more commonly, we are only interested in a statistic. A sample statistic or statistic is a quantitative measure calculated from a sample. For example the mean, the standard deviation, the minimum, the maximum, etc.\n",
    "\n",
    "Statistical models that assume that the distribution can be modeled with a finite set of parameters $\\theta = (\\theta_1, \\theta_2, ..., \\theta_k)$ are called parametric models. For example, if we assume that our data follows a normal distribution $N(\\mu, \\sigma^2)$ then $\\mu$ and $\\sigma$ are the parameters of the model. \n",
    "\n",
    "Frequentist models are based on the following postulates: \n",
    "\n",
    "1. Probability refers to limiting relative frequencies. Probabilities are objective properties of the real world.\n",
    "2. Parameters are fixed, unknown constants. Because they are not fluctuating, no useful probability statements can be made about parameters. \n",
    "3. Statistical procedures should be designed to have well-defined long run frequency properties. For example, a 95 percent confidence interval should trap the true value of the parameter with limiting frequency at least 95 percent. \n",
    "\n",
    "> Ref [Wasserman, 2013]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference\n",
    "\n",
    "The statistical methods that we have covered up to now requires that all probabilities be defined by connection to the frequencies of events in very large samples. This leads to frequentist uncertainty being premised on imaginary resampling of data. \n",
    "\n",
    "If we were to repeat the measurements many many times, we would en up collecting a list of values that will have some pattern to it. This means that parameters models cannot have probability distributions, only measurements can. \n",
    "\n",
    "The distribution on these measurements is called a sampling distribution but this resampling is usually never done or it doesn't even make sense. \n",
    "\n",
    "But there is another approach to inference called Bayesian inference which is based on the following postulates:\n",
    "\n",
    "1. Probability describes degree of belief, not limiting frequency. So we can make probability statements about a lots of things, not just data which are subject to random variation.\n",
    "2. We can make probability statements about parameters, even though they are fixed constants. \n",
    "3. We can make inferences about a parameter $\\theta$ by producing a probability distribution for it. \n",
    "4. Inferences, such as point estimates and interval estimates, may then be extracted from this distribution.\n",
    "\n",
    "In modest terms, Bayesian data analysis is no more than counting the number of ways the data could happen, according to our assumptions. \n",
    "\n",
    "In Bayesian analysis all alternative sequences of events that could have generated our data are evaluated. As we learn more about what happened, we prune some of this alternatives. By the end, what remains is only what is logically consistent with our knowledge.\n",
    "\n",
    "<!-- \n",
    "Grasping bayesian inference can be quite hard to understand, so lets see an example:\n",
    "\n",
    "- Let's suppose that we have a bag with four marbles. \n",
    "- These marbles come in two colors: blue and white.\n",
    "- We know there are four marbles in the bag, but we don't know how many are of each color.\n",
    "- But we can know that there are five possibilities:\n",
    "\n",
    "<img src=\"marbles_1.png\">\n",
    "\n",
    "- These are the only possibilities consistent with what we know about the contents of the bag. We will call this possibilities the conjectures. \n",
    "- Our goal is to figure out which of this conjectures is more plausible, given some evidence from the contents of the bag. \n",
    "- For our evidence, we pull a sequence of three marbles from the bag, one at a time and with replacement. \n",
    "- The sequence that we pull from the bag is [b,w,b]\n",
    "- Now, we can see how to use the data to infer what is in the bag.\n",
    "- Let's begin by considering just the single conjecture (2) [b,w,w,w], that the bag contains one blue and three white marbles. \n",
    "- So, on our first draw of the bag, one of four things could happen, corresponding to one of our four marbles in the bag.\n",
    "\n",
    "<img src=\"marbles_2.png\">\n",
    "-->\n",
    "\n",
    "A Bayesian model has the following components:\n",
    "\n",
    "- Parameter $\\theta$: A way of indexing possible explanations of the data. In our example $\\theta$ is a conjectured proportion of blue marbles.\n",
    "- Likelihood $f(d|\\theta)$: The relative number of ways that a value $\\theta$ can produce the data. It is derived by enumerating all the possible data sequences that could have happened and then eliminating those sequences inconsistent with the data.\n",
    "- Prior probability $f(\\theta)$: The prior plausibility of any specific value of $\\theta$.\n",
    "- Posterior probability $f(\\theta|d)$: The new, updated plausibility of any specific $\\theta$.\n",
    "- Evidence or Average Likelihood $f(d)$: the average probability of the data averaged over the prior. It’s job is just to standardize the posterior, to ensure it sums (integrates) to one.\n",
    "\n",
    "\n",
    "The general equation that relates all Bayesian components (for both density and mass functions) is the following:\n",
    "$$\n",
    "f(\\theta|d) = \\frac{f(d|\\theta) × f (\\theta)}{f(d)}\n",
    "\n",
    "$$\n",
    "\n",
    "This equation is essentially the Bayes theorem.\n",
    "It says that the probability of any particular value of $\\theta$ considering the data d, is proportional to the product of the relative plausibility of the data, conditional on $\\theta$, and the prior plausibility of $\\theta$.\n",
    "This product is then divided by the average probability of the data to produce a valid probability distribution for the posterior (to sum or integrate to one).\n",
    "We must bear in mind that Bayesian statistics is not only about using Bayes theorem. There are many non-Bayesian techniques that use this theorem.\n",
    "Bayesian inference uses the Bayes theorem more generally, to quantify uncertainty about unobserved variables such as parameters.\n",
    "\n",
    "\n",
    "\n",
    "> Ref [McElreath, 2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this sounds complicated but what can state of the art bayesian models do? \n",
    "\n",
    "Let's see some predictions made by spectral mixture multi-output gaussian processes: \n",
    "\n",
    "<img src=\"mogp_climate.png\">\n",
    "\n",
    "<img src=\"mogp_prediction.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this is not even a proper beginning of what Bayesian models are or can do. So once again I ask of you to check the references that I have mentioned up to now or some of the following that I will list. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Statistical Rethinking with brms, ggplot2, and the tidyverse](https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/)\n",
    "\n",
    "[Statistical Rethinking with Python and PyMC3](https://github.com/pymc-devs/resources/tree/master/Rethinking)\n",
    "\n",
    "[Statistical Rethinking with PyTorch and Pyro](https://fehiepsi.github.io/rethinking-pyro/)\n",
    "\n",
    "[Demystifying hypothesis testing with simple Python examples](https://towardsdatascience.com/demystifying-hypothesis-testing-with-simple-python-examples-4997ad3c5294)\n",
    "\n",
    "[On Bayesian and frequentist, latent variables and parameters By Dustin Tran](http://dustintran.com/blog/on-bayesian-and-frequentist-latent-variables-and-parameters)\n",
    "\n",
    "[What is statistics by Michael Jordan](https://www.youtube.com/watch?v=EYIKy_FM9x0&t=4742s)\n",
    "\n",
    "[Foundations of Statistics – Frequentist and Bayesian by Mary Parker](https://www.austincc.edu/mparker/stat/nov04/talk_nov04.pdf)\n",
    "\n",
    "[The Permutation Test by Jared Wilber](https://www.jwilber.me/permutationtest/)\n",
    "\n",
    "[Spurious Correlations](https://tylervigen.com/old-version.html)\n",
    "\n",
    "[Research Methods and Statistics Bristol University](http://www.bristol.ac.uk/medical-school/media/rms/red/index.html)\n",
    "\n",
    "[Seeing Theory - Frequentist Inference](https://seeing-theory.brown.edu/frequentist-inference/)\n",
    "\n",
    "[Stats with R by Manny Gimond](https://mgimond.github.io/Stats-in-R/index.html)\n",
    "\n",
    "[FiveMinuteStats by Matthew Stephens](https://stephens999.github.io/fiveMinuteStats/index.html)\n",
    "\n",
    "[Experimentation - Yale Universtity](http://www.stat.yale.edu/Courses/1997-98/101/expdes.htm)\n",
    "\n",
    "[Probabilistic graphical models notes by Stefano Ermon](https://ermongroup.github.io/cs228-notes/)\n",
    "\n",
    "[D-separation without tears by Judea Pearl](http://bayes.cs.ucla.edu/BOOK-2K/d-sep.html)\n",
    "\n",
    "[ML beyond Curve Fitting: An Intro to Causal Inference and do-Calculus by Ferenc Huszár](https://www.inference.vc/untitled/)\n",
    "\n",
    "[AIC and BIC by Barum Park](https://barumpark.com/blog/2018/aic-and-bic/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9 (default, Jun 29 2022, 11:45:57) \n[GCC 8.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
